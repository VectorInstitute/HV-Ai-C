model_output_dir: "./result_models"
wandb: True
agent:
  name: "QLearning"
  # Number of episodes 
  num_episodes: 5
  # Discount factor
  gamma: 0.99
  # Number of tiles for tile coding
  num_tiles: 20
  # Exploration initial rate
  initial_epsilon: 1.0
  # Initial value for Epsilon
  epsilon_annealing: 0.999
  # Initial learning rate
  learning_rate: 0.1
  # Annealing rate for learning rate
  lr_annealing: 0.999
  # Use HNP
  hnp: False
  # Number of episodes before logging
  log_interval: 1
    
env:
  # Sinergym environment name
  name: Eplus-5Zone-hot-discrete-train-v1
  # Reward Type
  reward_type: "Linear"
  # Whether to normalise observations
  normalize: True
  # The observation variables to use for training, offset by 4 to account for time vars 
  obs_to_keep: [4, 5, 13]
  # The type of each observation variable: 
  #   0 - slowly-changing continuous var
  #   1 - fast-changing continuous var
  #   2 - discrete var
  mask: [0, 0, 0]
