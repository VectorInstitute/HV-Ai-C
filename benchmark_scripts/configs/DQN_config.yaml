model_output_dir: "./result_models/"
wandb: True
agent:
  name: "DQN"
  # Number of episodes 
  num_episodes: 100
  # Discount factor
  gamma: 0.99
  # Initial value for Epsilon
  exploration_fraction: 0.12
  # Annealing rate for Epsilon
  exploration_final_eps: 0.1
  # Initial learning rate
  learning_rate: !!float 1e-4
  # Number of episodes before logging
  log_interval: 1
    
env:
  # Sinergym environment name
  name: Eplus-5Zone-hot-discrete-train-v1
  # Reward Type
  reward_type: "Linear"
  # Whether to normalise observations
  normalize: True
  # The observation variables to use for training, offset by 4 to account for time vars 
  obs_to_keep: [4, 5, 13]